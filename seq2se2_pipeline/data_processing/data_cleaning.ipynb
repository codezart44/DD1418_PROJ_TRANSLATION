{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert unicode files to ascii\n",
    "def __unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n",
    "\n",
    "\n",
    "def __preprocess_sentence(w: str) -> str: \n",
    "    \"\"\"@Re: String of input line only with alphabetical chars.\"\"\"\n",
    "    # w = __unicode_to_ascii(w.lower().strip())\n",
    "    w = w.strip().lower()\n",
    "\n",
    "    #creating a space between a word and the punctuation following it\n",
    "    # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r\"([.,!?])\", r\" \\1 \", w)\n",
    "    w = re.sub('\\s{2,}', ' ', w)\n",
    "\n",
    "    #replacing everything with space, except letters, punctuations ...\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,åäöÅÄÖ]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    #adding start and end tokens\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_clean_lines(src_file: str) -> None:\n",
    "    \"\"\"\"\"\"\n",
    "    print(f\"Getting from {src_file}\")\n",
    "    with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as srcf:\n",
    "        lines = srcf.readlines()\n",
    "        clean_lines = list(map(lambda l: __preprocess_sentence(w=l)+\"\\n\", lines))\n",
    "    return clean_lines\n",
    "\n",
    "\n",
    "def write_clean_lines(tgt_file: str, clean_lines: list[str]) -> None:\n",
    "    \"\"\"\"\"\"\n",
    "    assert tgt_file != \"europarl-v6.en\", \"Cannot overwrite large raw file.\"\n",
    "    assert tgt_file != \"europarl-v6.sv\", \"Cannot overwrite large raw file.\"\n",
    "\n",
    "    with open(file=tgt_file, mode=\"w\", encoding=\"utf-8\") as tgtf:\n",
    "        tgtf.writelines(clean_lines)\n",
    "\n",
    "\n",
    "def read_dataset(src_file, tgt_file) -> tuple[np.ndarray[str]]:\n",
    "    with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as swef:\n",
    "        swe_lines = swef.readlines()\n",
    "    with open(file=tgt_file, mode=\"r\", encoding=\"utf-8\") as engf:\n",
    "        eng_lines = engf.readlines()\n",
    "    return swe_lines, eng_lines\n",
    "\n",
    "\n",
    "def filter_lines_of_length(lines1: np.ndarray[str], lines2: list[str], max_length=40, min_length=5):\n",
    "    assert len(lines1) == len(lines2), \"Must be same length!\"\n",
    "    lines1_filtered = []\n",
    "    lines2_filtered = []\n",
    "    for i in range(len(lines1)):\n",
    "        if min_length < len(lines1[i].split(\" \")) < max_length and min_length < len(lines2[i].split(\" \")) < max_length:\n",
    "            if lines1[i] in lines1_filtered:\n",
    "                continue\n",
    "            lines1_filtered.append(lines1[i])\n",
    "            lines2_filtered.append(lines2[i])\n",
    "\n",
    "    return lines1_filtered, lines2_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting from ../../data/raw/europarl-v6.sv\n"
     ]
    }
   ],
   "source": [
    "local_dir_raw = \"../../data/raw/\"\n",
    "local_dir_cleaned = \"../../data/cleaned/\"\n",
    "\n",
    "eng_file = \"europarl-v6.en\"\n",
    "swe_file = \"europarl-v6.sv\"\n",
    "\n",
    "clean_eng_file = \"europarl-v6-cleaned.en\"\n",
    "clean_swe_file = \"europarl-v6-cleaned.sv\"\n",
    "\n",
    "clean_swe_lines = get_clean_lines(src_file=local_dir_raw+swe_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_clean_lines(tgt_file=local_dir_cleaned+clean_swe_file, clean_lines=clean_swe_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# clean_eng_file = \"europarl-v6-cleaned.en\"\n",
    "# clean_swe_file = \"europarl-v6-cleaned.sv\"\n",
    "# swe_lines, eng_lines = read_dataset(src_file=local_dir_cleaned+clean_swe_file, tgt_file=local_dir_cleaned+clean_eng_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 12\n",
    "# MIN_LENGTH = 5\n",
    "# swe_lines_filtered, eng_lines_filtered = filter_lines_of_length(lines1=swe_lines, lines2=eng_lines, max_length=MAX_LENGTH, min_length=MIN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_filtered_eng_file = \"europarl-v6-cleaned-filtered.en\"\n",
    "# clean_filtered_swe_file = \"europarl-v6-cleaned-filtered.sv\"\n",
    "\n",
    "# write_clean_lines(tgt_file=local_dir_cleaned+clean_filtered_eng_file, clean_lines=eng_lines_filtered)\n",
    "# write_clean_lines(tgt_file=local_dir_cleaned+clean_filtered_swe_file, clean_lines=swe_lines_filtered)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
